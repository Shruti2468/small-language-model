{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5c66885930744e7955b087ad5dcb493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad27f420eacc4f6cb3fd16759030d08e",
              "IPY_MODEL_f91aa0cd6737441992bea8ecbcc3fd16",
              "IPY_MODEL_ecab4ca36ece4125b46d592d8614d02c"
            ],
            "layout": "IPY_MODEL_162f9e8b595546cf80b2ef5b0964d31b"
          }
        },
        "ad27f420eacc4f6cb3fd16759030d08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a37dea69d3b3461190a586fb3e8757e0",
            "placeholder": "​",
            "style": "IPY_MODEL_eb7cb81524b444af8d65238a978dd0f3",
            "value": "tokenizing the splits (num_proc=8):   5%"
          }
        },
        "f91aa0cd6737441992bea8ecbcc3fd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da82cb4db4024949a3eb119113fd0f28",
            "max": 2119719,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a430792436334258b2289abaeeb2224d",
            "value": 105238
          }
        },
        "ecab4ca36ece4125b46d592d8614d02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7373d4e4a6a741939d0da78fadefc4a5",
            "placeholder": "​",
            "style": "IPY_MODEL_6b4f632bbf5a44a281113b5902e07f2c",
            "value": " 105238/2119719 [00:45&lt;13:46, 2438.28 examples/s]"
          }
        },
        "162f9e8b595546cf80b2ef5b0964d31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a37dea69d3b3461190a586fb3e8757e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7cb81524b444af8d65238a978dd0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da82cb4db4024949a3eb119113fd0f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a430792436334258b2289abaeeb2224d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7373d4e4a6a741939d0da78fadefc4a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b4f632bbf5a44a281113b5902e07f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YfEgXSq3jHp",
        "outputId": "0ea00111-e85b-42dd-e259-0d80dc1e9244",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# import tinystories dataset\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "AZ8AsU7B3o6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that the dataset is installed  we need to tokenize the dataset\n",
        "vocbulary -> list of all the words , higher vocublury size more computation power required\n",
        " types of token( has words , characters and subwords ) uses the stem of uncommon words or splits it in to sub words which are not reppeated to reduce the vocabulary size\n",
        "byte pair encoding gpt 2-> gives token to the amount of vocabularies you want -> this is wat is used here\n",
        "\n",
        "each token has a token id\n",
        "store the tokens in .bin file\n"
      ],
      "metadata": {
        "id": "ebRBPeKx31Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "etwruoAF4Cty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff22c1c-0ff1-479d-bea2-1a5e1f0018a6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken #(bpe library)\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm #progress bar for loops\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def get_ids(story):\n",
        "  ids= enc.encode_ordinary(story[\"text\"])\n",
        "  output={\"ids\":ids,\"len\":len(ids)}\n",
        "  #enc.encode(text)          # normal encode, respects special tokens\n",
        "  #enc.encode_ordinary(text) # ignores special tokens   choose based on the type of dataset\n",
        "  return output\n",
        "\n",
        "if not os.path.exists (\"train.bin\"):\n",
        "  tokenized = ds.map(\n",
        "      get_ids,\n",
        "      remove_columns=[\"text\"],# no need to text column anymore only need the ids\n",
        "      desc=\"tokenizing the splits\", #progress bar label\n",
        "      num_proc=8  #runs parallel across 8 CPU processes\n",
        "  )\n",
        "\n",
        "  for split, dset in tokenized.items():\n",
        "    #[(\"train\", train_dataset), (\"val\", val_dataset)] gives output like this\n",
        "    # we are using uint -> unsigned int since these tokens cannot be negative values\n",
        "    arr_len = np.sum(dset['len'],dtype=np.uint64)\n",
        "    filename= f'{split}.bin'\n",
        "    arr= np.memmap(filename,dtype = np.uint16, mode='w+',shape=(arr_len,))\n",
        "    total_batches=1024 #batch will contain roughly 1/1024 to avoid memory overload\n",
        "\n",
        "    idx=0\n",
        "    for batch_idx in tqdm(range(total_batches),desc=f'writing {filename}'):\n",
        "      batch = dset.shard(num_shards=total_batches,index=batch_idx, contiguous=True).with_foramt('numpy')\n",
        "      #.with_foramt('numpy')-> converts dataset shard to NumPy arrays, which are faster for numerical operations\n",
        "      #contiguous=True -> ensures the examples in the shard are taken consecutively from the dataset, not randomly scattered.\n",
        "      arr_batch=np.concatenate(batch['ids'])\n",
        "      arr[idx:idx+len(arr_batch)] = arr_batch\n",
        "      idx +=len(arr_batch)\n",
        "    arr.flush()\n",
        "    #arr.flush() forces all pending changes to be written to the actual .bin file on disk.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "f5c66885930744e7955b087ad5dcb493",
            "ad27f420eacc4f6cb3fd16759030d08e",
            "f91aa0cd6737441992bea8ecbcc3fd16",
            "ecab4ca36ece4125b46d592d8614d02c",
            "162f9e8b595546cf80b2ef5b0964d31b",
            "a37dea69d3b3461190a586fb3e8757e0",
            "eb7cb81524b444af8d65238a978dd0f3",
            "da82cb4db4024949a3eb119113fd0f28",
            "a430792436334258b2289abaeeb2224d",
            "7373d4e4a6a741939d0da78fadefc4a5",
            "6b4f632bbf5a44a281113b5902e07f2c"
          ]
        },
        "id": "R1JkITO_Gwn7",
        "outputId": "9b9c13c5-ad48-4cc7-db23-98c489b1a31f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5c66885930744e7955b087ad5dcb493"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkPoolWorker-4:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/managers.py\u001b[0m in \u001b[0;36m_callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3309\u001b[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001b[0m\u001b[1;32m   3310\u001b[0m                             \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2289882187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"train.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   tokenized = ds.map(\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mget_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# no need to text column anymore only need the ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             dataset_dict[split] = dataset.map(\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3304\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3305\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3306\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Spawning {num_proc} processes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'terminating pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    223\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    730\u001b[0m                     \u001b[0;31m# worker has not yet exited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaning up worker %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/multiprocess/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Input-Output batches for the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "Uk18pF8qTX93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split): #returns one input and one output matrix\n",
        "  if split == 'train':\n",
        "    data = np.memmap('train.bin',dtype=np.unit16,mode='r')\n",
        "  else :\n",
        "    data = np.memmap('validation.bin',dtype=np.unit16,mode='r')\n",
        "\n",
        "  xi= torch.randint(len(data) -block_size, (batch_size,)) # (batch_size,) -> size (tuple) – a tuple defining the shape of the output tensor.\n",
        "  x= torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in xi])\n",
        "  # creates a batch of input sequences (x) by stacking token slices of length block_size from random positions in the dataset.\n",
        "  y= torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in xi])\n",
        "\n",
        "  if device_type == \"cuda\":\n",
        "    x,y =x.pin_memory().to(device, non_blocking =True),y.pin_memory().to(device, non_blocking =True)\n",
        "    # (x.pin_memory()) locks the CPU memory for these tensors, so the GPU can access it directly without extra copying\n",
        "    # (non_blocking=True) lets the CPU continue running other tasks while the data is being copied to the GPU.\n",
        "  else:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "6RGcNHu11tqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SLM Model Architecture\n",
        "\n",
        "3 main steps\n",
        "-> input\n",
        "-> processor\n",
        "-> output\n",
        "\n",
        "INPUT\n",
        "Converts all the token ids to embedding vectors.\n",
        "The embedding vectors are stored in a token embedding matrix where (x, y) — x = size of vocab and y is an experimental value.\n",
        "\n",
        "This token embedding matrix acts as a lookup table for all the ids, so if an id is entered then the corresponding row is fetched.\n",
        "\n",
        "The token embedding vectors are all randomly initialized and can be trained later.\n",
        "\n",
        "IN INPUT: TOKENIZED TEXT -> TOKEN EMBEDDINGS -> POSITIONAL EMBEDDINGS\n",
        "\n",
        "Positional embeddings are added so the model knows the order of tokens in the sequence, since attention alone does not encode positions.\n",
        "\n",
        "PROCESSOR:\n",
        "\n",
        "Layer Normalization keeps token activations stable after embeddings or attention layers, preventing exploding or vanishing gradients.\n",
        "\n",
        "Self-attention -> computes each token in a sentence with respect to all the other tokens, capturing relationships between words.\n",
        "\n",
        "Multi-head attention -> looks at different aspects of the context in parallel. After this step, the dimension size stays the same, but the vectors now contain richer information about the context.\n",
        "\n",
        "We divide by sqrt of keys dimension to scale the dot product and keep values in a normal range, preventing extremely large numbers.\n",
        "\n",
        "After we get the attention heads, we do causal attention, which masks the upper half of the attention matrix (sets them to -inf so after softmax they become 0), ensuring that tokens cannot “see” future words. This is essential for autoregressive training, otherwise the model would cheat.\n",
        "\n",
        "Softmax is applied to get attention probabilities, and dropout can also be applied to improve generalization.\n",
        "\n",
        "The attention weights are multiplied with the values matrix to get context vectors. Another dropout can be applied here.\n",
        "\n",
        "A small neural network can be added at the end (optional) with dropout for additional processing.\n",
        "\n",
        "Shortcut connections are also added, similar to ResNet, allowing the input of a layer to go directly to the next layer without going through multi-head attention and dropout. This helps prevent vanishing gradient problems when multiple layers are stacked.\n",
        "\n",
        "Layer normalization ensures the output has mean = 0 and variance = 1.\n",
        "\n",
        "Next is the feed-forward (FF) network:\n",
        "\n",
        "A FF neural network is applied.The feed-forward network looks at each token on its own and improves its features, helping the model understand each token better after attention.\n",
        "\n",
        "GELU activation function is applied (experimental choice).\n",
        "\n",
        "Dropout layer can be added for regularization.\n",
        "\n",
        "OUTPUT :\n",
        "final normalixzation layer\n",
        "output\n",
        "gives the logits matrix for next prediction task\n",
        "Each number in this vector is a logit, which is the model’s “score” for how likely each word in the vocabulary is the next token.\n"
      ],
      "metadata": {
        "id": "KW9iIlvG8o36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#  nn.Module is the base class for all neural network layers and models in PyTorch, handling parameter storage, registration, and forward computation.\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclass import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  # The learnable parameters (weight and bias) are stored inside your LayerNorm (nn) object and automatically registered and managed by PyTorch’s nn.Module system.\n",
        "  def __init__(self,ndim,bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias =nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  def forward(self,x):\n",
        "    return F.layer_norm(x,self.weight.shape,self.weight,self.bias,1e-5) #1e-5 added to denominator to ensure its never zero\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    # ensures that n_embd can be cleanly divided into equal parts\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3* config.n_embd,bias=config.bias) #computes queries (Q), keys (K), and values (V) at once.\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd,bias=config.bias) # Linear layer after attention to project the concatenated heads back to original embedding dimension.\n",
        "    self.attn_dropout=nn.Dropout(config.dropout)\n",
        "    self.resid_dropout=nn.Dropout(config.dropout) # dropout after the residual connection.\n",
        "    self.n_head =config.n_head\n",
        "    self.n_embd =config.n_embd\n",
        "    # Saves the number of heads and embedding dimension for later use.\n",
        "    self.flash =hasattr(F,'scaled_dot_product_attention')\n",
        "    if not self.flash :\n",
        "      self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size , config.block_size))\n",
        "\n",
        "    #  lower-triangular matrix (torch.tril) to mask out future tokens in attention.\n",
        "    #  Shape [1,1,block_size,block_size] allows broadcasting across batches and heads.\n",
        "    # .view() reshapes a tensor without copying data, requiring it to be contiguous\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.size()\n",
        "    q,k,v = self.c_attn(x).split(self.n_embd,dim=2)\n",
        "    k=k.view(B,T, self.n_head , C//self.n_head).transpose(1,2)\n",
        "    q=q.view(B,T, self.n_head , C//self.n_head).transpose(1,2)\n",
        "    v =v.view(B,T, self.n_head , C//self.n_head).transpose(1,2)\n",
        "\n",
        "    if self.flash :\n",
        "      y= F.scaled_dot_product_attention(q,k,v,attn_mask=None,dropout_p=self.attn_dropout.p if self.training else 0.0 , is_causal=True)\n",
        "      # This line computes causal scaled dot-product attention by combining queries, keys, and values, applying softmax, optional dropout, and masking future tokens, producing the attended output y.\n",
        "    else:\n",
        "      att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1))) #“scaled” dot-product attention.\n",
        "      att = att.masked_fill(self.bias[:,:,:T,:T]== 0 , float('-inf'))\n",
        "      # This line applies the causal mask to the attention scores, setting future positions to -inf so they get zero probability after softmax.\n",
        "      att = F.softwax(att,dim=-1)\n",
        "      att = self.attn_dropout(att)\n",
        "      y= att@v\n",
        "\n",
        "    y=y.transpose(1,2).contiguous().view(B,T,C)\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    # applied to the residual connection\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    # is the first linear layer of the Transformer feed-forward block, expanding the embedding dimension by 4× before applying a non-linear activation.\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "  def forward(self, x):\n",
        "      return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "      # shortcut connection or residual connection\n",
        "      # We need the shortcut connection to preserve original information Because without it, each layer would overwrite the input, causing information loss and\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int # Maximum context length\n",
        "    vocab_size: int\n",
        "    n_layer: int # Number of Transformer blocks\n",
        "    n_head: int\n",
        "    n_embd: int # hidden size of each token vector.\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "      wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "      # embedding lookup table -> Converts each input token ID into a dense vector of dimension\n",
        "      wpe=nn.Embedding(config.block_size, config.n_embd),# Positional Embedding\n",
        "      drop=nn.Dropout(config.dropout),\n",
        "      h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),#block class\n",
        "      ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    # maps the final token embeddings to logits over the vocabulary\n",
        "    self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "    # The vector used to represent a token as input is the same vector used to score it as output.\n",
        "    self.apply(self._init_weights)\n",
        "    # Recursively goes through all submodules of the model (all layers, blocks, embeddings, etc.) and calls _init_weights(module) on each.\n",
        "    for pn, p in self.named_parameters():\n",
        "        if pn.endswith('c_proj.weight'):\n",
        "            nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "            # We start the attention projection weights smaller so that the model doesn’t blow up when it’s deep.\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):#Checks if the layer is a fully connected (Linear) layer.\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02) #Initializes the Linear layer’s weights with small random numbers from a normal distribution.\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):# Checks if the layer is an Embedding layer.\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02) # Initializes all token vectors in the Embedding layer with small random numbers.\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      device = idx.device\n",
        "      b, t = idx.size()\n",
        "      assert t <= self.config.block_size\n",
        "      pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "      tok_emb = self.transformer.wte(idx) # Looks up the token embeddings for each input ID.\n",
        "      pos_emb = self.transformer.wpe(pos)\n",
        "      x = self.transformer.drop(tok_emb + pos_emb)\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "      # Passes x through each Transformer block (h = stack of Block).\n",
        "      # Each block applies self-attention + feedforward + residual connections.\n",
        "      x = self.transformer.ln_f(x)\n",
        "\n",
        "      if targets is not None:\n",
        "          logits = self.lm_head(x) # Projects final embeddings x to vocabulary logits for each token.\n",
        "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "          # Computes cross-entropy loss between predicted logits and target token IDs.\n",
        "\n",
        "          return logits, loss\n",
        "      else:\n",
        "          logits = self.lm_head(x[:, [-1], :])\n",
        "          return logits, None\n",
        "\n",
        "\n",
        "    @torch.no_grad() # Tells PyTorch not to compute gradients during this function.\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):# maximum number of tokens we generate\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # Keep only the last block_size tokens if the sequence is too long, otherwise use the full sequence.\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Pass the current sequence through GPT to get predicted scores for the next token.\n",
        "            logits = logits[:, -1, :] / temperature # Selects the last token’s logits from the sequence. we only care about the next token after the current sequence.\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                # Find the top-k highest scores from the logits to limit candidate tokens for sampling.\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "                # Set all logits outside the top-k to negative infinity so they cannot be sampled.\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Convert the logits into probabilities for all tokens so we can randomly sample the next token.\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Randomly pick the next token from the probability distribution predicted by the model.\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "CNMvUq678nwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "outputId": "15892db3-b0d1-4f47-d354-0c29a72d8c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (ipython-input-1739638768.py, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1739638768.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    def __init__(self, config)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ],
      "metadata": {
        "id": "bIqEAXuzmXWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def estimate_loss(model):\n",
        "  out ={}\n",
        "  model.eval()\n",
        "  # Sets the model to evaluation mode:Disables dropout. Freezes batch normalization updates.ensures the loss reflects inference-time behavior, not training randomness.\n",
        "  with torch.inference_model():\n",
        "    for split in ['train','val']:\n",
        "      losses= torch.zeros(eval_iters) #Creates a tensor to store loss values for each mini-batch .\n",
        "      for k in range(eval_iters):\n",
        "        X, Y = get_batch(split)\n",
        "        with ctx:\n",
        "          logits,loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "AAcvcnx-ZYHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if out outoput is i had alwasy though ans the coresspoindgin logits are given we get thr correspoding toke id romt his , ex fro token id of 23 to be max if token id 23 contas the correct next word to the current word, so we neeed the propablitly of 23 to be high  after soft max\n",
        "\n",
        "when we get softmax of the vocab list after the traing so if we have word had then after the softwmax we need to make sure always which is the next correct word has the highest propbailty\n",
        "\n",
        "flateen , multiple outputs are clubbed toger\n",
        "cress_entrpoy loss -taking negative log likehood for ther tokens we have\n",
        "\n",
        "estiome loss -> extimates losses per batch this loss  and we taken mean of losses , we run the model for a prescribed no of iteraations , for each iteration we get diff batch we pass abtch through mode and get logit and pass the logits throuigjh the model and we get the loss ,\n",
        "from logits we get the tokens whic coressponsds to the decoded next token\n",
        "for sentence  \"i had always thought\" the output will be \"had always thougth that\"\n",
        "\n",
        "eaCH WORD SHOULD HAVE THE PROBALITYL OF THE NEXT\n",
        "\n",
        " WOORD\n",
        "\n",
        "logits= 4 by vocab size\n",
        "flattend all logits matrix togeter into one big logits"
      ],
      "metadata": {
        "id": "rVfKEia-dGNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE SLM TRAINING CONFIG"
      ],
      "metadata": {
        "id": "o3MaZe0RkZ9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training , earlier 1e-3\n",
        "max_iters = 20000  # limits the number of times the model’s weights are updated. ,no of training iterations\n",
        "warmup_steps = 100\n",
        "# The model starts updating weights immediately, from step 1.\n",
        "# What warmup does is start with a very small learning rate at step 1 and gradually increase it over the first 1000 steps.\n",
        "# By step 1000, the learning rate reaches its full intended value. After that, training continues normally with the full learning rate.\n",
        "min_lr = 5e-4\n",
        "# min_lr = 5e-4 → the learning rate won’t go lower than 0.0005.\n",
        "eval_iters = 500 #number of times the model is evaluated during training\n",
        "batch_size = 32 #number of samples the model sees before it updates the weights once.\n",
        "block_size = 128 # the number of tokens (words/characters) the model looks at in one input sequence. context size\n",
        "\n",
        "gradient_accumulation_steps = 32\n",
        "# Normally, the model updates weights after every batch. With gradient accumulation, the model waits for 32 batches, adds up the gradients, and then updates weights once.This is like saving up small steps before taking a bigger step.\n",
        "\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "# chooses the “mode” for calculations depending on whether you are on CPU or GPU.\n",
        "\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "LwJCUChukYSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(),lr=learning_rate, betas =(0.9,0.95),weight_decay=0.1,eps=1e-9)\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "#Essentially, the GradScaler temporarily boosts the tiny gradients so they don't disappear in float16, and then it corrects the boost just in time for the update."
      ],
      "metadata": {
        "id": "soOnx-Ww1sSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inpit, ouput  -> llm -> logits-> loss -> loss propagated backawrasd and so on\n",
        "\n",
        "gradient accumulation -> if gpu can only accpt 32 samples\n",
        "so we can d updated paramter after every 32 steps instead of 1024 step , so we run 32 iteratoins so we do in batches of 32 so not doingparameter update after 32 instead of 1024\n",
        "\n",
        "gradient_accumulation_steps = 32\n",
        " accumulating greadtuis untill this points then updating\n",
        "\n",
        "\n",
        " we use adamoptimizer with weight decay\n",
        "\n",
        " for learning rate we have linear warmup ie initially then we have a cosine decay\n",
        "\n",
        "\n",
        " In Adam (and AdamW), two moving averages are maintained:\n",
        "\n",
        "m_t → the first moment (mean of past gradients)\n",
        "\n",
        "v_t → the second moment (mean of squared gradients)\n",
        "\n",
        "\n",
        "weight_decay=0.1 → Adds regularization by slightly shrinking large weights toward zero.\n",
        "\n",
        "\n",
        "\n",
        "eps=1e-9\n",
        "the average of squared gradient becomes very small, the denominator could approach zero → causing division by zero or huge updates.\n",
        "\n",
        "Adding a small constant eps (like 1e-9) ensures that doesn’t happen.\n",
        "\n",
        "β₁ controls how much past gradient direction (momentum) is remembered, while β₂ controls how much past gradient magnitude (squared gradients) is remembered for adaptive learning rates."
      ],
      "metadata": {
        "id": "emMii8dbv3Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "Ahj61625DNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IJ9POqDmDTlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ],
      "metadata": {
        "id": "2SgW_qIDDU0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "7tLlOb2qDZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "input goes to model  , i get decoded token from trained model , decoded token is appended to input goes to model and so on"
      ],
      "metadata": {
        "id": "o1L5wgv9Dp7-"
      }
    }
  ]
}